<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">



  
  
    
    
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link href="/lib/pace/pace-theme-flash.min.css?v=1.0.2" rel="stylesheet">







<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.2" rel="stylesheet" type="text/css">


  <meta name="keywords" content="nlp,MRC,Deep learning,SQuAD,">








  <link rel="shortcut icon" type="image/x-icon" href="/images/favicon.jpeg?v=5.1.2">






<meta name="description" content="​    本文仅供作者复习使用,大量参考了Danqi Chen的博士论文 1.阅读理解1.1 什么是RC?​    阅读理解是近两年自然语言处理领域的研究热点之一,受到学术界和工业界的广泛关注.而阅读理解,即指让机器阅读文本,然后根据文本回答问题,对机器的文本理解能力、推理能力甚至一些人类常识都有一定的要求.">
<meta name="keywords" content="nlp,MRC,Deep learning,SQuAD">
<meta property="og:type" content="article">
<meta property="og:title" content="机器阅读理解(MRC) 和 SQuAD">
<meta property="og:url" content="http://yoursite.com/2019/02/24/ReadcomprehensionAndSQuAD/index.html">
<meta property="og:site_name" content="Shenge&#39;s Blog">
<meta property="og:description" content="​    本文仅供作者复习使用,大量参考了Danqi Chen的博士论文 1.阅读理解1.1 什么是RC?​    阅读理解是近两年自然语言处理领域的研究热点之一,受到学术界和工业界的广泛关注.而阅读理解,即指让机器阅读文本,然后根据文本回答问题,对机器的文本理解能力、推理能力甚至一些人类常识都有一定的要求.">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://yoursite.com/2019/02/24/ReadcomprehensionAndSQuAD/MCTest.png">
<meta property="og:image" content="http://yoursite.com/2019/02/24/ReadcomprehensionAndSQuAD/google_search.png">
<meta property="og:image" content="http://yoursite.com/2019/02/24/ReadcomprehensionAndSQuAD/SQuAD_example.png">
<meta property="og:image" content="http://yoursite.com/2019/02/24/ReadcomprehensionAndSQuAD/SQuAD_leaderboard.png">
<meta property="og:image" content="http://yoursite.com/2019/02/24/ReadcomprehensionAndSQuAD/MARCO_leaderboard.png">
<meta property="og:image" content="http://yoursite.com/2019/02/24/ReadcomprehensionAndSQuAD/CoQA.png">
<meta property="og:image" content="http://yoursite.com/2019/02/24/ReadcomprehensionAndSQuAD/dataset.png">
<meta property="og:image" content="http://yoursite.com/2019/02/24/ReadcomprehensionAndSQuAD/timeline.png">
<meta property="og:image" content="http://yoursite.com/2019/02/24/ReadcomprehensionAndSQuAD/interface.png">
<meta property="og:image" content="http://yoursite.com/2019/02/24/ReadcomprehensionAndSQuAD/reasoning.png">
<meta property="og:image" content="http://yoursite.com/2019/02/24/ReadcomprehensionAndSQuAD/answer.png">
<meta property="og:image" content="http://yoursite.com/2019/02/24/ReadcomprehensionAndSQuAD/comparition.png">
<meta property="og:image" content="http://yoursite.com/2019/02/24/ReadcomprehensionAndSQuAD/FQA_net.jpg">
<meta property="og:image" content="http://yoursite.com/2019/02/24/ReadcomprehensionAndSQuAD/BiDAF.png">
<meta property="og:image" content="http://yoursite.com/2019/02/24/ReadcomprehensionAndSQuAD/R-Net.png">
<meta property="og:image" content="http://yoursite.com/2019/02/24/ReadcomprehensionAndSQuAD/QANet.png">
<meta property="og:image" content="http://yoursite.com/2019/02/24/ReadcomprehensionAndSQuAD/speedup.png">
<meta property="og:updated_time" content="2019-04-08T08:35:03.985Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="机器阅读理解(MRC) 和 SQuAD">
<meta name="twitter:description" content="​    本文仅供作者复习使用,大量参考了Danqi Chen的博士论文 1.阅读理解1.1 什么是RC?​    阅读理解是近两年自然语言处理领域的研究热点之一,受到学术界和工业界的广泛关注.而阅读理解,即指让机器阅读文本,然后根据文本回答问题,对机器的文本理解能力、推理能力甚至一些人类常识都有一定的要求.">
<meta name="twitter:image" content="http://yoursite.com/2019/02/24/ReadcomprehensionAndSQuAD/MCTest.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    sidebar: {"position":"left","display":"always","offset":12,"offset_float":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2019/02/24/ReadcomprehensionAndSQuAD/">





  <title>机器阅读理解(MRC) 和 SQuAD | Shenge's Blog</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  














</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Shenge's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/02/24/ReadcomprehensionAndSQuAD/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="北冥有深">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/xiongmao.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Shenge's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">机器阅读理解(MRC) 和 SQuAD</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-02-24T15:55:46+08:00">
                2019-02-24
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/MRC/" itemprop="url" rel="index">
                    <span itemprop="name">MRC</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                <span class="post-meta-divider">|</span>
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计</span>
                
                <span title="字数统计">
                  4.8k
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长</span>
                
                <span title="阅读时长">
                  20
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>​    <font size="5"><center>本文仅供作者复习使用,大量参考了Danqi Chen的<a href="https://cs.stanford.edu/~danqi/papers/thesis.pdf" target="_blank" rel="noopener">博士论文</a></center></font></p>
<h1 id="1-阅读理解"><a href="#1-阅读理解" class="headerlink" title="1.阅读理解"></a>1.阅读理解</h1><h2 id="1-1-什么是RC"><a href="#1-1-什么是RC" class="headerlink" title="1.1 什么是RC?"></a>1.1 什么是RC?</h2><p>​    阅读理解是近两年自然语言处理领域的研究热点之一,受到学术界和工业界的广泛关注.而阅读理解,即指让机器阅读文本,然后根据文本回答问题,对机器的文本理解能力、推理能力甚至一些人类常识都有一定的要求.</p>
<a id="more"></a>
<h2 id="1-2-为什么是RC"><a href="#1-2-为什么是RC" class="headerlink" title="1.2 为什么是RC?"></a>1.2 为什么是RC?</h2><p>教机器去理解人类语言是一个很难的事情,那我们为什么会选择去做阅读理解任务?</p>
<p><img src="MCTest.png" alt></p>
<center>图1: 来自MCTest数据集的一个例子</center>

<p>其实,NLP社区为了解决语言理解问题已经设计并考虑了很多任务:</p>
<ul>
<li><strong>词性标注</strong>:如<em>Alyssa got to the beach after a long trip</em>, Alyssa是专有名词, got是过去式动词, long是形容词, after是介词等等.</li>
<li><strong>命名实体识别</strong>: 需要知道例如<em>Alyssa, Ellen, Kristen</em>是人名; <em>Charlotte, Atlanta and Miami</em>是地名等等.</li>
<li><strong>句法分析</strong>: <em>Alyssa got to the beach after a long trip</em> 中<em>Alyssa</em>是主语, <em>beach</em>是go的目标, <em>after a long trip</em>是修饰动词的介词短语等等.</li>
<li><strong>指代消解</strong>: 需要去理解<em>She</em>,<em>he</em>一类词的指代对象.</li>
</ul>
<p>然后人们发现,阅读理解,阅读文章并回答问题,能够有效的考察这些方面考察机器对人类语言的理解.</p>
<ul>
<li><p><strong>(a)</strong> 第一个问题<em>What city is Alyssa in?</em> ,机器需挑出<em>She’s now in Miami</em>并知道She指的是<em>Alyssa</em></p>
</li>
<li><p><strong>(b)</strong> 第二个问题<em>What did Alyssa eat at the restaurant?</em>  机器需要挑出<em>The restaurant had a special on catfish.</em> 和 <em>Alyssa enjoyed the restaurant’s special</em>这两个句子并且知道<em>special</em>指的是<em>catfish</em>.</p>
</li>
<li><p><strong>(c)</strong> 第三个问题<em>How many friends does Alyssa have in this story?</em> 就更有挑战性,机器需要追踪文中所有的人名并分析它们的关系,需要很强的理解推理能力.</p>
</li>
</ul>
<p>所以, 阅读理解任务可能是最适合评估机器对文本理解能力的任务.</p>
<p>并且,阅读理解在人类的生活,实际应用中能起到很大的作用.</p>
<p><img src="google_search.png" alt></p>
<center>图2: 使用Google搜索答案的同时,google在返回网页的同时给出了更精准的答案</center>

<p>​    例如使用搜索引擎搜索问题时, 利用阅读理解技术,我们可以直接返回给用户答案而不仅是检索出来的网页.而且,现在常用的语言助手例如Amazon的Alexa, Apple的siri等语音助手, 人们也经常使用他们来问一些常识性的或者信息类的问题, 而阅读理解技术无疑于是对话和问答中的关键. </p>
<p>结合阅读理解技术为关键发展出很多应用,Danqi Chen的<a href="https://cs.stanford.edu/~danqi/papers/thesis.pdf" target="_blank" rel="noopener">博士论文</a>中提出了下面两类:</p>
<ul>
<li><strong>开放领域问答</strong>: 结合信息检索技术和阅读理解技术来回答一些通识性问题.</li>
<li><strong>对话式问答</strong>: 结合对话领域和问答领域的挑战, 把一轮的问答转变为关于一段文本的对话式问题. </li>
</ul>
<p>总而言之, 阅读理解的主要应用场景集中在机器对非结构化文本内容的阅读和理解上.</p>
<h2 id="1-3-阅读理解问题的分类"><a href="#1-3-阅读理解问题的分类" class="headerlink" title="1.3 阅读理解问题的分类"></a>1.3 阅读理解问题的分类</h2><h3 id="1-3-1-Cloze-style-完形填空式"><a href="#1-3-1-Cloze-style-完形填空式" class="headerlink" title="1.3.1 Cloze-style(完形填空式)"></a>1.3.1 Cloze-style(完形填空式)</h3><p>​    <font size="5"><center><a href="https://research.fb.com/downloads/babi/" target="_blank" rel="noopener">CBT数据集</a></center></font></p>
<p>​    CBT数据集全称是Children’s Book Test,数据集是facebook从儿童读物中自动生成的,每21句话构成一个example,前20句是用于阅读的上下文,第21句会被抽走一个单词,机器要从给定的十个词中去预测哪个词才是被抽走的词,根据被抽走的词可以将问题分成四类：命名实体、名词、动词和介词.</p>
<p>​    更详细的关于数据集的介绍请查看下面一篇论文:    </p>
<p>Felix Hill, Antoine Bordes, Sumit Chopra and Jason Weston. <a href="https://research.fb.com/publications/the-goldilocks-principle-reading-childrens-books-with-explicit-memory-representations/" target="_blank" rel="noopener">The Goldilocks Principle: Reading Children’s Books with Explicit Memory Representations</a>, arXiv:1511.02301.</p>
<p>   一个example:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">Context:</span><br><span class="line">1 So they had to fall a long way .	</span><br><span class="line">2 So they got their tails fast in their mouths .	</span><br><span class="line">3 So they could n&apos;t get them out again .	</span><br><span class="line">4 That &apos;s all .	</span><br><span class="line">5 `` Thank you , &quot; said Alice , `` it &apos;s very interesting .	</span><br><span class="line">6 I never knew so much about a whiting before . &quot;	</span><br><span class="line">7 `` I can tell you more than that , if you like , &quot; said the Gryphon .	</span><br><span class="line">8 `` Do you know why it &apos;s called a whiting ? &quot;	</span><br><span class="line">9 `` I never thought about it , &quot; said Alice .	</span><br><span class="line">10 `` Why ? &quot;	</span><br><span class="line">11 `` IT DOES THE BOOTS AND SHOES . &apos;	</span><br><span class="line">12 the Gryphon replied very solemnly .	</span><br><span class="line">13 Alice was thoroughly puzzled .	</span><br><span class="line">14 `` Does the boots and shoes ! &quot;	</span><br><span class="line">15 she repeated in a wondering tone .	</span><br><span class="line">16 `` Why , what are YOUR shoes done with ? &quot;	</span><br><span class="line">17 said the Gryphon .	</span><br><span class="line">18 `` I mean , what makes them so shiny ? &quot;	</span><br><span class="line">19 Alice looked down at them , and considered a little before she gave her answer .	</span><br><span class="line">20 `` They &apos;re done with blacking , I believe . &quot;	</span><br><span class="line"></span><br><span class="line">Query: `` Boots and shoes under the sea , &quot; the XXXXX went on in a deep voice , `` are done with a whiting &quot;.</span><br><span class="line"></span><br><span class="line">Candidates: Alice|BOOTS|Gryphon|SHOES|answer|fall|mouths|tone|way|whiting	</span><br><span class="line">	</span><br><span class="line">Answer: gryphon</span><br></pre></td></tr></table></figure>
<ul>
<li>其他的还有如<a href="https://cs.nyu.edu/~kcho/DMQA/" target="_blank" rel="noopener">CNN/Daily Mail 数据集</a>,由DeepMind从真实的新闻数据中自动标注得到的,具体细节查看网页链接.</li>
</ul>
<h3 id="1-3-2-多选式"><a href="#1-3-2-多选式" class="headerlink" title="1.3.2 多选式"></a>1.3.2 多选式</h3><p>​    多选式阅读理解非常好理解,和我们中学做过的英语阅读一样,给一篇文章阅读,然后回答关于这篇文章的选择题.</p>
<font size="5"><center><a href="https://www.microsoft.com/en-us/research/publication/mctest-challenge-dataset-open-domain-machine-comprehension-text/" target="_blank" rel="noopener">MCTest</a></center></font>

<p>一个example:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">James the Turtle was always getting in trouble.Sometimes he&apos;d reach into the freezer and empty out all the food. Other times he&apos;d sled on the deck and get a splinter. His aunt Jane tried as hard as she could to keep him out of trouble, but he was sneaky and got into lots of trouble behind her back.</span><br><span class="line">One day, James thought he would go into town and see what kind of trouble he could get into. He went to the grocery store and pulled all the pudding off the shelves and ate two jars. Then he walked to the fast food restaurant and ordered 15 bags of fries. He didn&apos;t pay, and instead headed home.</span><br><span class="line">His aunt was waiting for him in his room. She told James that she loved him, but he would have to start acting like a well-behaved turtle.</span><br><span class="line">After about a  month, and after getting into lots of trouble, James finally made up his mind to be a better turtle.</span><br><span class="line"></span><br><span class="line">1) What is the name of the trouble making turtle?</span><br><span class="line">A) Fries</span><br><span class="line">B) Pudding</span><br><span class="line">C) James</span><br><span class="line">D) Jane</span><br><span class="line">2) What did James pull off of the shelves in the grocery store?</span><br><span class="line">A) pudding</span><br><span class="line">B) fries</span><br><span class="line">C) food</span><br><span class="line">D) splinters</span><br><span class="line">3) Where did James go after he went to the grocery</span><br><span class="line">store?</span><br><span class="line">A) his deck</span><br><span class="line">B) his freezer</span><br><span class="line">C) a fast food restaurant</span><br><span class="line">D) his room</span><br><span class="line">4) What did James do after he ordered the fries?</span><br><span class="line">A) went to the grocery store</span><br><span class="line">B) went home without paying</span><br><span class="line">C) ate them</span><br><span class="line">D) made up his mind to be a better turtle</span><br></pre></td></tr></table></figure>
<font size="5"><center><a href="http://www.qizhexie.com/data/RACE_leaderboard" target="_blank" rel="noopener">RACE数据集</a></center></font>

<p>​    RACE数据集是从中学生和高中生的英语考试中的阅读理解题中而来的.</p>
<p>​    详情请看以下论文:</p>
<p>Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, Eduard Hovy.<a href="https://arxiv.org/abs/1704.04683" target="_blank" rel="noopener">RACE: Large-scale ReAding Comprehension Dataset From Examinations</a> arXiv:1704.04683</p>
<h3 id="1-3-3-抽取式"><a href="#1-3-3-抽取式" class="headerlink" title="1.3.3 抽取式"></a>1.3.3 抽取式</h3><p>​    抽取式数据集,即给定一篇文章,给一个问题,答案则是从文章中抽取出来的一段连续的文字,可以是一个词也可以是多个词.</p>
<font size="5"><center><a href="https://rajpurkar.github.io/SQuAD-explorer/" target="_blank" rel="noopener">SQuAD</a></center></font>

<p><img src="SQuAD_example.png" alt></p>
<p>​    <center>图3 : 一个段落和对应的问题、答案</center></p>
<p>​        SQuAD由斯坦福大学发布,是行业内公认的机器阅读理解领域的顶级水平测试,在学术界和工业界有着非常高的认可度和广泛的影响力,是阅读理解乃至自然语言处理领域里比较权威的数据集.</p>
<p>​    SQuAD从维基百科中选取了500篇文章,每篇文章划分成一些段落,每个段落有若干个问题,以此构建了一个包含十万个问题的大规模机器阅读理解数据集,其问题和答案都是让crowdworkers进行人工生成.</p>
<p>​    SQuAD16年6月发布1.0版本,18年6月发布2.0版本.2.0版本和1.0版本的区别主要在于2.0版本多了很多根据文章无法回答的问题,所以机器需要去判断根据给定的文章能不能回答问题,这也让2.0版本的数据集增加了难度.但从实际应用角度考虑,2.0版本的数据集显然更实用更贴近生产应用实际,机器也需要判断问题能不能回答.</p>
<p><img src="SQuAD_leaderboard.png" alt></p>
<center>图4 : SQuAD2.0的leaderboard</center>

<p>​    稍后会详细介绍SQuAD数据集,以下是数据集的论文链接:</p>
<p>SQuAD 1.0: <a href="https://arxiv.org/abs/1606.05250" target="_blank" rel="noopener">SQuAD: 100,000+ Questions for Machine Comprehension of Text</a></p>
<p>SQuAD 2.0: <a href="https://arxiv.org/abs/1806.03822" target="_blank" rel="noopener">Know What You Don’t Know: Unanswerable Questions for SQuAD</a></p>
<font size="4"><center><strong>评价指标</strong></center></font>

<ul>
<li><strong>Exact match(EM)</strong>: predicted answer和ground truth之间是否相等</li>
<li><strong>F1 score</strong>: 计算predicted answer和ground truth之间的单词重叠, 有多个答案时计算最大的答案的F1 score再进行平均</li>
</ul>
<p>$$  F1 = \frac{2 \times Precision \times Recall}{Precision + Recall} $$</p>
<h3 id="1-3-4-综合式"><a href="#1-3-4-综合式" class="headerlink" title="1.3.4 综合式"></a>1.3.4 综合式</h3><font size="5"><center><a href="http://www.msmarco.org/leaders.aspx" target="_blank" rel="noopener">MS MARCO</a></center></font>

<p>​    MARCO 数据集中的问题全都基于来自微软必应搜索（BING）引擎和微软小娜人工智能助手（Cortana）的已匿名处理的真实查询.此外,相关回答是由真人参考真实网页编写的,并对其准确性进行了验证.而在每一个问题中,MARCO 提供多篇来自搜索结果的网页文档,系统需要根据这些文档来回答给定的问题.就像人类在搜索引擎给定的结果中自行筛选信息一样,这些文档中是否有对应的答案、在哪一篇文章中,都需要系统自行判断,甚至还需要结合多篇文章做出提炼与总结,而这也对机器的阅读理解能力提出了更高的要求.</p>
<p><img src="MARCO_leaderboard.png" alt></p>
<center>图5 : MS MARCO的leaderboard</center>

<font size="4"><center><strong>评价指标</strong></center></font>

<p>没有一个consensus的评价指标, 用natural language generation (NLG) 的指标例如<strong>BLEU</strong>, <strong>Meteor</strong>等代替</p>
<h3 id="1-3-5-对话式"><a href="#1-3-5-对话式" class="headerlink" title="1.3.5 对话式"></a>1.3.5 对话式</h3><font size="5"><center><a href="https://stanfordnlp.github.io/coqa/" target="_blank" rel="noopener">CoQA</a></center></font>

<p><img src="CoQA.png" alt></p>
<center>图6 : 一个CoQA数据example的格式</center>

<p>对话式阅读理解数据集主要结合了阅读理解和多轮对话的共同特点, 机器需要去阅读一段文本,并据此回答一系列相关的问题, 这些问题是相关的, 因此需要机器去编码多轮对话之间的联系. </p>
<h3 id="1-3-6-一些数据集的总结"><a href="#1-3-6-一些数据集的总结" class="headerlink" title="1.3.6 一些数据集的总结"></a>1.3.6 一些数据集的总结</h3><p><img src="dataset.png" alt></p>
<center>图7:不同类别任务对应的数据集(黄色为中文)</center>

<h2 id="1-4-发展历程"><a href="#1-4-发展历程" class="headerlink" title="1.4 发展历程"></a>1.4 发展历程</h2><p><img src="timeline.png" alt></p>
<center>图8: 阅读理解领域重要进展时间线, 蓝色是模型, 黑色是数据集</center>

<p>阅读理解领域的发展大致有三个阶段,在早期时,模型大多是传统的语言学系统;2013-2015年间,由于有了 MCTest, PROCESSBANK 等更多的监督学习数据集, 逐渐地过渡到rule-based的机器学习系统,这些系统较强地依赖于设计特征的好坏;2015至今,伴随着深度学习浪潮的到来,阅读理解领域全面由端到端的深度学习模型接管.</p>
<p>最近阅读理解领域的巨大成功主要归功于两点:</p>
<ul>
<li>更大规模更高质量数据集的提出</li>
<li>神经网络阅读理解模型的发展</li>
</ul>
<p>神经网络阅读理解模型相比传统的rule-based模型的优点:</p>
<ul>
<li>神经网络阅读理解模型不依赖于下游的语言学系统或工具例如dependency分析器emantic role labling(语义角色标注)系统等, 这些工具往往有很多不完善的地方并且存在泛化性差等问题,提取出来的特征噪声也会很大.</li>
<li>神经网络模型自动学习所有的特征, 而这些特征也很难被手动有效的设计出来, 人们也可以把更多的精力放在设计网络结构上</li>
</ul>
<h1 id="2-SQuAD"><a href="#2-SQuAD" class="headerlink" title="2. SQuAD"></a>2. SQuAD</h1><h2 id="2-1-数据集特点"><a href="#2-1-数据集特点" class="headerlink" title="2.1 数据集特点"></a>2.1 数据集特点</h2><p>​    前文提到过了SQuAD的数据集格式和影响力,下面主要介绍一些数据集的细节和特点.</p>
<ul>
<li>问题(query)的复杂性</li>
</ul>
<p>因为问题是由人类提出的,而且他们在被要求提出问题时就被要求问题要尽可能复杂并且问题与原文使用的词汇要尽可能不重叠,这导致了回答问题需要一定的推理能力,比如问题和原文中对应答案的段落之间存在词汇替换,这种替换可能是简单的同义词,可能是需要外部知识来理解,也可能出现语法、句式的不同,甚至需要多个句子联合推理等等.</p>
<p><img src="interface.png" alt></p>
<center>图9: 构造众包数据集的网站接口</center>

<p><img src="reasoning.png" alt></p>
<center>图10: 回答问题需要多种不同的推理知识</center>

<ul>
<li>答案的多样性</li>
</ul>
<p>答案是从原文中抽取出来的片段,可能是一个词也可能是多个词,可能是日期、其他数字、人名、地名、名词短语、动词短语等等各种类型,答案的多样性也增加了阅读理解的难度.</p>
<p><img src="answer.png" alt></p>
<center>图11: 答案的种类</center>

<h2 id="2-2-SQuAD-2-0"><a href="#2-2-SQuAD-2-0" class="headerlink" title="2.2 SQuAD 2.0"></a>2.2 SQuAD 2.0</h2><p>SQuAD2.0是在1.0共10w个问题的基础上增加了5w个新的根据上下文没法回答的问题.机器在阅读寻找答案的同时,不仅要判断最可能的位置,还要判断到底问题可不可以被回答,这种改进更符合实际的阅读理解需求但也增加了不少难度.</p>
<p><img src="comparition.png" alt></p>
<center>图12: 一些模型在1.0和2.0数据集上的对比, 模型和人类的差距在2.0数据集上显著增大</center>

<h2 id="2-3-模型通用架构"><a href="#2-3-模型通用架构" class="headerlink" title="2.3 模型通用架构"></a>2.3 模型通用架构</h2><p><img src="FQA_net.jpg" alt></p>
<center>图13: SQuAD问题的pipeline</center>

<p>大部分围绕SQuAD的模型架构基本上可以分为四个层,embedding层、encode层、interaction层、answer层.</p>
<ul>
<li>embedding层 : 将文章和问题中的tokens映射成向量表示,可以是word层面的embedding,也可以包含character层面的embedding.</li>
<li>encoder层 : 利用RNN对文章和问题向量进行编码,获得上下文相关的词汇的向量表示.</li>
<li>interaction层 : 主要利用注意力机制去编码文章和问题向量,捕捉问题和文章之间的交互关系,生成包含了问题语义信息的原文表示. </li>
<li>answer层 : 基于编码过的文章向量和问题向量来预测答案范围,一般预测答案的start和end两个位置.</li>
</ul>
<h2 id="2-4经典模型"><a href="#2-4经典模型" class="headerlink" title="2.4经典模型"></a>2.4经典模型</h2><h3 id="2-4-1-BiDAF-Bidirectional-Attention-Flow-Model"><a href="#2-4-1-BiDAF-Bidirectional-Attention-Flow-Model" class="headerlink" title="2.4.1 BiDAF(Bidirectional Attention Flow Model)"></a>2.4.1 BiDAF(Bidirectional Attention Flow Model)</h3><p><img src="BiDAF.png" alt></p>
<ul>
<li><strong>Character Embedding Layer</strong>: 利用CNN获得每个单词的character-level的表示, 对处理out-of-vacabluary的单词会有帮助.</li>
<li><strong>Word Embedding Layer</strong>: 使用Glove预训练词向量获得单词的word-level的表示.</li>
<li><strong>Contextual Embedding Layer</strong>: 使用LSTM对文章和问题分别进行编码,获得基于上下文的单词表示</li>
<li><strong>Attention Flow Layer</strong>: 通过query-to-context和context-to-query的双向注意力计算,获得query-awared的context的表示</li>
<li><strong>Modeling Layer</strong>: 利用LSTM进行进一步的信息融合和建模.</li>
<li><strong>Output Layer</strong>: 通过Dense+softmax获得start位置和end位置的概率.</li>
</ul>
<p>BiDAF主要的创新点在于利用了static的注意力计算和双向注意力机制, 通过提前利用context和query的向量计算出相似度矩阵,再对矩阵进行行列的概率归一化即可获得双向的注意力.</p>
<h3 id="2-4-2-R-Net"><a href="#2-4-2-R-Net" class="headerlink" title="2.4.2 R-Net"></a>2.4.2 R-Net</h3><p><img src="R-Net.png" alt></p>
<p>R-Net主要的创新点在于在context-to-query的interaction层之后,又添加了self-attention机制, 来更好地进行长距离的信息匹配和融合.</p>
<h3 id="2-4-3-QANet"><a href="#2-4-3-QANet" class="headerlink" title="2.4.3 QANet"></a>2.4.3 QANet</h3><p><img src="QANet.png" alt></p>
<p>QANet在架构上和其他模型仍是一样的, 它主要是考虑到RNN网络训练和推理的速度都很慢,训练很慢会导致很难快速迭代调优模型和使用更大的数据集,推理很慢影响系统部署到对实时性要求高的场景; 因此,它提出了一种新的encoder block, 利用卷积和self-attention来进行编码,编码器内部的结构组成类似transformer, 不同的是增加了卷积模块.</p>
<p>其主要贡献在于:</p>
<ul>
<li>提出了基于卷积和self-attention的新型编码结构, 替换了RNN, 大大加速了训练过程.</li>
<li>提出了一种叫backtranslation的数据增强方法,即把原文翻译成另外一种语言再翻译回来,作为数据增强.</li>
</ul>
<p>其加速效果如下图所示:</p>
<p><img src="speedup.png" alt></p>
<h1 id="3-参考文献"><a href="#3-参考文献" class="headerlink" title="3.参考文献"></a>3.参考文献</h1><p>[1] Richardson M, Burges C J, Renshaw E. Mctest: A challenge dataset for the open-domain machine comprehension of text[C]//Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. [S.l.: s.n.], 2013: 193-203.<br>[2] Chen D. Neural reading comprehension and beyond[D]. [S.l.]: Stanford University, 2018.<br>[3] Hill F, Bordes A, Chopra S, et al. The goldilocks principle: Reading children’s books with explicit memory representations[J]. arXiv preprint arXiv:1511.02301, 2015.<br>[4] Hermann K M, Kocisky T, Grefenstette E, et al. Teaching machines to read and comprehend[C]//Advances in Neural Information Processing Systems. [S.l.: s.n.], 2015: 1693-1701.<br>[5] Lai G, Xie Q, Liu H, et al. Race: Large-scale reading comprehension dataset from examinations[J]. arXiv preprint arXiv:1704.04683, 2017.<br>[6] Rajpurkar P, Zhang J, Lopyrev K, et al. Squad: 100,000+ questions for machine comprehension of text[J]. arXiv preprint arXiv:1606.05250, 2016.<br>[7] Rajpurkar P, Jia R, Liang P. Know what you don’t know: Unanswerable questions for squad[J]. arXiv preprint arXiv:1806.03822, 2018.<br>[8] Nguyen T, Rosenberg M, Song X, et al. Ms marco: A human generated machine reading comprehension dataset[J]. arXiv preprint arXiv:1611.09268, 2016.<br>[9] Chen D, Bolton J, Manning C D. A thorough examination of the cnn/daily mail reading comprehension task[J]. arXiv preprint arXiv:1606.02858, 2016.<br>[10] Seo M, Kembhavi A, Farhadi A, et al. Bidirectional attention flow for machine comprehension[J]. arXiv preprint arXiv:1611.01603, 2016.<br>[11] Xiong C, Zhong V, Socher R. Dynamic coattention networks for question answering [J]. arXiv preprint arXiv:1611.01604, 2016.<br>[12] Wang W, Yang N, Wei F, et al. Gated self-matching networks for reading comprehension and question answering[C]//Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers): volume 1. [S.l.:s.n.], 2017: 189-198.<br>[13] Wang S, Jiang J. Machine comprehension using match-lstm and answer pointer[J]. arXiv preprint arXiv:1608.07905, 2016.<br>[14] Yu A W, Dohan D, Luong M T, et al. Qanet: Combining local convolution with global self-attention for reading comprehension[J]. arXiv preprint arXiv:1804.09541, 2018.<br>[15] Kim Y. Convolutional neural networks for sentence classification[J]. arXiv preprint arXiv:1408.5882, 2014.<br>10[16] Devlin J, Chang M W, Lee K, et al. Bert: Pre-training of deep bidirectional transformers for language understanding[J]. arXiv preprint arXiv:1810.04805, 2018.<br>[17] Joshi M, Choi E, Weld D S, et al. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension[J]. arXiv preprint arXiv:1705.03551, 2017.<br>[18] Shen Y, Huang P S, Gao J, et al. Reasonet: Learning to stop reading in machine comprehension[C]//Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. [S.l.]: ACM, 2017: 1047-1055.<br>[19] Weissenborn D, Wiese G, Seiffe L. Making neural qa as simple as possible but not simpler[J]. arXiv preprint arXiv:1703.04816, 2017.<br>[20] Mikolov T, Sutskever I, Chen K, et al. Distributed representations of words and phrases and their compositionality[C]//Advances in neural information processing systems. [S.l.: s.n.], 2013: 3111-3119.<br>[21] Pennington J, Socher R, Manning C. Glove: Global vectors for word representation[C]// Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP). [S.l.: s.n.], 2014: 1532-1543.<br>[22] Bojanowski P, Grave E, Joulin A, et al. Enriching word vectors with subword infor- mation[J]. Transactions of the Association for Computational Linguistics, 2017, 5: 135-146.<br>[23] Hochreiter S, Schmidhuber J. Long short-term memory[J]. Neural computation, 1997,9(8): 1735-1780.<br>[24] Cho K, Van Merriënboer B, Gulcehre C, et al. Learning phrase representation susing rnn encoder-decoder for statistical machine translation[J]. arXiv preprint arXiv:1406.1078, 2014.<br>[25] Schuster M, Paliwal K K. Bidirectional recurrent neural networks[J]. IEEE Transactions on Signal Processing, 1997, 45(11): 2673-2681.<br>[26] Sutskever I, Vinyals O, Le Q V. Sequence to sequence learning with neural networks [C]//Advances in neural information processing systems. [S.l.: s.n.], 2014: 3104-3112.<br>[27] Bahdanau D, Cho K, Bengio Y. Neural machine translation by jointly learning to align and translate[J]. arXiv preprint arXiv:1409.0473, 2014.<br>[28] Luong M T, Pham H, Manning C D. Effective approaches to attention-based neural machine translation[J]. arXiv preprint arXiv:1508.04025, 2015.<br>[29] Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need[C]//Advances in 11Neural Information Processing Systems. [S.l.: s.n.], 2017: 5998-6008.</p>

      
    </div>
    
    
    

  <div>
    
      <div>
    
        <div style="text-align:center;color: #555;font-size:14px;">-------------The End-------------</div>
    
</div>

    
  </div>

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/nlp/" rel="tag"># nlp</a>
          
            <a href="/tags/MRC/" rel="tag"># MRC</a>
          
            <a href="/tags/Deep-learning/" rel="tag"># Deep learning</a>
          
            <a href="/tags/SQuAD/" rel="tag"># SQuAD</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/09/23/pythonLearning/" rel="next" title="Python基础知识整理(from 廖雪峰)">
                <i class="fa fa-chevron-left"></i> Python基础知识整理(from 廖雪峰)
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/03/05/thesis-Match-LSTM/" rel="prev" title="论文分享---Match-LSTM">
                论文分享---Match-LSTM <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image" src="/images/xiongmao.jpeg" alt="北冥有深">
          <p class="site-author-name" itemprop="name">北冥有深</p>
           
              <p class="site-description motion-element" itemprop="description">这世界一切既然都在变 <br> 变动中人世乘除 <br>自然就有些 <br> 近于偶然与凑巧的事情发生<br> 哀乐与悲欢<br> 都有他独特的式样</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives/">
                <span class="site-state-item-count">9</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">4</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">15</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#1-阅读理解"><span class="nav-text">1.阅读理解</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-1-什么是RC"><span class="nav-text">1.1 什么是RC?</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-2-为什么是RC"><span class="nav-text">1.2 为什么是RC?</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-3-阅读理解问题的分类"><span class="nav-text">1.3 阅读理解问题的分类</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-1-Cloze-style-完形填空式"><span class="nav-text">1.3.1 Cloze-style(完形填空式)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-2-多选式"><span class="nav-text">1.3.2 多选式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-3-抽取式"><span class="nav-text">1.3.3 抽取式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-4-综合式"><span class="nav-text">1.3.4 综合式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-5-对话式"><span class="nav-text">1.3.5 对话式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-6-一些数据集的总结"><span class="nav-text">1.3.6 一些数据集的总结</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-4-发展历程"><span class="nav-text">1.4 发展历程</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2-SQuAD"><span class="nav-text">2. SQuAD</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#2-1-数据集特点"><span class="nav-text">2.1 数据集特点</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-2-SQuAD-2-0"><span class="nav-text">2.2 SQuAD 2.0</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-3-模型通用架构"><span class="nav-text">2.3 模型通用架构</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-4经典模型"><span class="nav-text">2.4经典模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-1-BiDAF-Bidirectional-Attention-Flow-Model"><span class="nav-text">2.4.1 BiDAF(Bidirectional Attention Flow Model)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-2-R-Net"><span class="nav-text">2.4.2 R-Net</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-3-QANet"><span class="nav-text">2.4.3 QANet</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#3-参考文献"><span class="nav-text">3.参考文献</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

      <div id="music163player">
       <iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="280" height="86" src="//music.163.com/outchain/player?type=2&id=494858498&auto=1&height=66"></iframe>
      </div>
    </div>
    
  </aside>
  




        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">北冥有深</span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Gemini
  </a>
</div>


        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>

  
  <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.2"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.2"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.2"></script>



  


  




	





  





  






  





  

  

  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
  


  

  

</body>
</html>
