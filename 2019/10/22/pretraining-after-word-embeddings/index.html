<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">



  
  
    
    
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link href="/lib/pace/pace-theme-flash.min.css?v=1.0.2" rel="stylesheet">







<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.2" rel="stylesheet" type="text/css">


  <meta name="keywords" content="nlp,pretraining,ELMo,GPT,BERT,XLNet,ALBert,">








  <link rel="shortcut icon" type="image/x-icon" href="/images/favicon.jpeg?v=5.1.2">






<meta name="description" content="NLP预训练发展史   1.背景介绍NLP领域的预训练技术主要关注在词语的语义表征上。那么，我们首先看看nlp的representation learning。所以，我们应该怎么样去表征一个词汇呢？最简单的，可以用one-hot向量来代替一个词语，即长度为词表大小的向量，用其中特定一维为1来表示特定的词。但显然，这样有很多缺点：  one-hot表征缺少语义信息，任意词汇向量之间都是正交的没有任何">
<meta name="keywords" content="nlp,pretraining,ELMo,GPT,BERT,XLNet,ALBert">
<meta property="og:type" content="article">
<meta property="og:title" content="NLP预训练发展史">
<meta property="og:url" content="http://yoursite.com/2019/10/22/pretraining-after-word-embeddings/index.html">
<meta property="og:site_name" content="Shenge&#39;s Blog">
<meta property="og:description" content="NLP预训练发展史   1.背景介绍NLP领域的预训练技术主要关注在词语的语义表征上。那么，我们首先看看nlp的representation learning。所以，我们应该怎么样去表征一个词汇呢？最简单的，可以用one-hot向量来代替一个词语，即长度为词表大小的向量，用其中特定一维为1来表示特定的词。但显然，这样有很多缺点：  one-hot表征缺少语义信息，任意词汇向量之间都是正交的没有任何">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://yoursite.com/2019/10/22/pretraining-after-word-embeddings/td_matrix.png">
<meta property="og:image" content="http://yoursite.com/2019/10/22/pretraining-after-word-embeddings/tt_matrix.png">
<meta property="og:image" content="http://yoursite.com/2019/10/22/pretraining-after-word-embeddings/tf_formula.png">
<meta property="og:image" content="http://yoursite.com/2019/10/22/pretraining-after-word-embeddings/idf_formula.png">
<meta property="og:image" content="http://yoursite.com/2019/10/22/pretraining-after-word-embeddings/tf_idf_formula.png">
<meta property="og:image" content="http://yoursite.com/2019/10/22/pretraining-after-word-embeddings/td_matrix.png">
<meta property="og:image" content="http://yoursite.com/2019/10/22/pretraining-after-word-embeddings/tf_idf_matrix.png">
<meta property="og:image" content="http://yoursite.com/2019/10/22/pretraining-after-word-embeddings/lm.png">
<meta property="og:image" content="http://yoursite.com/2019/10/22/pretraining-after-word-embeddings/nnlm.png">
<meta property="og:image" content="http://yoursite.com/2019/10/22/pretraining-after-word-embeddings/word2vec.png">
<meta property="og:image" content="http://yoursite.com/2019/10/22/pretraining-after-word-embeddings/negative_sampling.png">
<meta property="og:image" content="http://yoursite.com/2019/10/22/pretraining-after-word-embeddings/negative_sampling_formula.png">
<meta property="og:updated_time" content="2019-10-26T14:57:51.457Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="NLP预训练发展史">
<meta name="twitter:description" content="NLP预训练发展史   1.背景介绍NLP领域的预训练技术主要关注在词语的语义表征上。那么，我们首先看看nlp的representation learning。所以，我们应该怎么样去表征一个词汇呢？最简单的，可以用one-hot向量来代替一个词语，即长度为词表大小的向量，用其中特定一维为1来表示特定的词。但显然，这样有很多缺点：  one-hot表征缺少语义信息，任意词汇向量之间都是正交的没有任何">
<meta name="twitter:image" content="http://yoursite.com/2019/10/22/pretraining-after-word-embeddings/td_matrix.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    sidebar: {"position":"left","display":"always","offset":12,"offset_float":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2019/10/22/pretraining-after-word-embeddings/">





  <title>NLP预训练发展史 | Shenge's Blog</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  














</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Shenge's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/10/22/pretraining-after-word-embeddings/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="北冥有深">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/xiongmao.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Shenge's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">NLP预训练发展史</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-10-22T19:21:59+08:00">
                2019-10-22
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/nlp/" itemprop="url" rel="index">
                    <span itemprop="name">nlp</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                <span class="post-meta-divider">|</span>
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计</span>
                
                <span title="字数统计">
                  2.7k
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长</span>
                
                <span title="阅读时长">
                  9
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <font size="5"><center>NLP预训练发展史</center></font>

<p><br><br></p>
<h1 id="1-背景介绍"><a href="#1-背景介绍" class="headerlink" title="1.背景介绍"></a>1.背景介绍</h1><p>NLP领域的预训练技术主要关注在<strong>词语的语义表征</strong>上。那么，我们首先看看nlp的<strong>representation learning</strong>。所以，我们应该怎么样去表征一个词汇呢？最简单的，可以用<strong>one-hot向量</strong>来代替一个词语，即长度为词表大小的向量，用其中特定一维为1来表示特定的词。但显然，这样有很多缺点：</p>
<ul>
<li>one-hot表征缺少语义信息，任意词汇向量之间都是正交的没有任何相似性。</li>
<li>one-hot表征向量稀疏且巨大，消耗存储和计算资源。</li>
</ul>
<p>那么，我们自然想寻找一种单词的向量化表示，它最好有以下优点：</p>
<a id="more"></a>
<ul>
<li>词向量维度不要太大，避免资源浪费。</li>
<li>词向量能具有一定的语义属性，简单点就是，两个向量的余弦距离可以用来表征两个词语语义之间的相似程度，余弦值越大，语义相似程度越大。我们期望这个词向量能具有很好的语义属性，能分辨同义词和反义词，知道词的相似程度和相关程度，知道词的隐含意义，比如能告诉我们猫和狗很像，告诉我们冷和热相反，等等等等。</li>
</ul>
<p>想要找到好的单词表征，我们还是得去思考一个词的含义究竟由什么确定。在1953年，哲学家Ludwig Wittgenstein提出<strong>“the meaning of a word is its use in the language”</strong>，同期的一些语言学家Joos (1950), Harris (1954), and Firth (1957) 提出：<strong>define a word by its environment or distribution in language use</strong>，其idea是出现在更相似语境中的词更倾向于具有相同的语义，这听起来就很合理！</p>
<p>一个很自然的想法是通过统计一个词周围出现过什么词以及词的个数来作为这个词的语义表征，这蕴含着两方面想法：一是通过上下文出现的词汇来表示单词的语义，二是用一系列数字，一个向量，语义空间中的一个点来表征一个词汇。因此，用向量来表示词汇也被称为word embeddings, 即词被embedded在向量空间中。</p>
<p>下面介绍两种词汇的向量表征模型，<strong>tf-idf</strong>模型和<strong>word2vec</strong>模型：</p>
<h2 id="Co-occurence-matrix"><a href="#Co-occurence-matrix" class="headerlink" title="Co-occurence matrix"></a>Co-occurence matrix</h2><p>从distributonal hypothesis很自然引申出来的一个概念就是co-occurence矩阵，也分为两种，term-document matrix和term-term(word-word) matrix。下图是term-document matrix, 这个矩阵就是在各个document中统计出现词的个数，矩阵的行可以作为term的特征，列可以作为document的特征，但是一般作为document的特征使用更常见，可以用于IR信息检索领域查找最匹配的document。<img src="td_matrix.png" alt="1571834239686"></p>
<p>term-term co-occurence matrix则更常用于作为词的特征，矩阵大小一般是|V|*|V|。</p>
<p><img src="tt_matrix.png" alt></p>
<h2 id="Tf-idf-模型"><a href="#Tf-idf-模型" class="headerlink" title="Tf-idf 模型"></a>Tf-idf 模型</h2><p>考虑term-document matrix, 我们想讲term出现的频率作为document的特征，但是这合理吗？有一些词比如good在所有的document中都出现了很多次，但是像fool就只在部分document中出现，那么显然fool就比good更加有判别力，是个更重要的特征。</p>
<p>首先是term frequency:</p>
<p><img src="tf_formula.png" alt></p>
<p>很好理解，就是term在document中出现的次数再压缩一下。</p>
<p>然后是inverse document frequency:</p>
<p><img src="idf_formula.png" alt></p>
<p>N是所有document的个数，df是某个词某个term出现过的document的个数，比如下图中document总共就四个,N=4,good在四篇文章中都出现过，因此df=4，idf计算出来等于0。</p>
<p>最后：</p>
<p><img src="tf_idf_formula.png" alt></p>
<p>想法就是，我现在想要用document出现了哪些单词作为特征，但如果有一个单词你所有document都出现了，那么我认为你这个特征对于区分document没啥帮助，我就舍弃这个特征。</p>
<p><img src="td_matrix.png" alt="1571834239686"></p>
<p><img src="tf_idf_matrix.png" alt></p>
<p>利用co-occurence matrix矩阵作为词汇特征还是有很多问题，比如词汇表通常很大，所以特征维度就会很大，并且容易出现稀疏问题，因此并不是一种很好的特征表示方法。</p>
<h2 id="Word2vec"><a href="#Word2vec" class="headerlink" title="Word2vec"></a>Word2vec</h2><p>下面我们要使用一个short(50-1000维)并且dense(大多数维非0)的向量来作为单词的表征，这样的向量维度小容易学习并且dense的属性也好易于捕捉一些语义属性。</p>
<p>词嵌入的概念最早提出是在Bengio在2003年发表在JMLR上的论文，是用神经网络来训练语言模型。语言模型最初是用来评估一个句子的合理性，即它出现的概率或者说是一个合理的句子的概率有多大，可以用来在语音识别解码时挑选最合理的句子，一个句子的概率可以看成一堆条件概率的乘积：</p>
<script type="math/tex; mode=display">P(s) = p(w_1)p(w_2|w_1)p(w_3|w_1w_2)...p(w_n|w_1w_2...w_{n-1})​</script><p>我们对概率进行一定的近似：</p>
<p><img src="lm.png" alt></p>
<p>即下一个词出现的概率只跟前面的n个词相关。</p>
<p>那么，如果要设计一个神经网络来做语言模型，我们该怎么做？下图是Bengio的做法。</p>
<p><img src="nnlm.png" alt></p>
<p>把每个词的one-hot乘以词嵌入矩阵，也就是|V|*|d|大小的参数矩阵，转换成词向量，再继续过两层全连接进行分类。E矩阵和U矩阵都可以用来做词嵌入矩阵。其实Bengio的本意是想做神经网络语言模型，词嵌入矩阵只是副产品。</p>
<p>而2013年word2vec现世，word2vec和NNLM架构类似，只不过训练的目的就是为了获取词向量矩阵，因此它提出了两种训练任务和两种训练方法。两种训练任务是CBOW和skip-gram，CBOW的想法就是根据上下文词去预测中心词，skip-gram的想法就是根据中心词去预测周围词的分布。两种训练方法是negative sampling和hierarchical softmax, 后一种不常用还麻烦，主要介绍skip-gram with negative sampling(SGNS)。</p>
<p><img src="word2vec.png" alt></p>
<p>skip-gram原始的做法是center word的one-hot乘以输入词向量矩阵获得中心词的词向量，再拿中心词的词向量和输出词向量矩阵中的每个词内积作为相似分数，最后进行softmax得到概率分布。这样的问题就是每个训练样例都需要进行|V|次内积操作，运算量非常大。因此作者提出了negative sampling。与直接预测周围词的分布不同，我们选取一个center word和另外一个word，判断另外一个word是不是在center word的周围，这样训练一次只需要进行一次向量内积，实际训练的时候，真实的周围词作为正样例，再根据词频进行采样出随机词作为负样例。如下图所示：</p>
<p><img src="negative_sampling.png" alt></p>
<p>采样负样例的计算公式：</p>
<p><img src="negative_sampling_formula.png" alt></p>
<p>$\alpha$一般0.75，负样例的个数作者建议对小数据集，一个正样例配5-20个负样例，大数据集则配2-5个负样例。</p>
<h1 id="2-后词向量时代"><a href="#2-后词向量时代" class="headerlink" title="2.后词向量时代"></a>2.后词向量时代</h1><p>总结一下上述的内容，用神经网络学习word embedding就是利用语言模型这一训练任务去训练得到词向量矩阵，每个词有其对应的词向量，以此来迁移到下游的nlp任务中作为一个token的单独的语义表征。但为什么word embedding之前没有那么火热呢？主要是因为多义词问题。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1.我喜欢吃苹果。</span><br><span class="line">2.我喜欢用苹果的产品。</span><br></pre></td></tr></table></figure>
<p>由于word embedding针对每一个特定的词只产生一个词向量，所以苹果公司和水果苹果的语义势必会混淆在一个语义向量里，对于其本身的语义和下游任务来说都是一个很大的问题。对于多义词问题，也有很多研究人员提出了解决方案，但ELMo以一种更简洁优雅的方法解决了多义词问题。后续像GPT、BERT、XLNet等预训练模型也相继涌现，成就了现在NLP预训练领域的后词向量时代。</p>
<p>关于预训练来说，这个给预训练下一个简介的定义：</p>
<font size="4"><center>预训练就是通过设定预训练任务，把训练数据里的语言学知识装载到模型里。</center></font>

<p>所以，重要的其实就是<strong>预训练任务</strong>和<strong>预训练模型</strong>(特征提取器)。就预训练任务来说，XLNet的论文里把现在的语言学预训练任务分成了<strong>Autoregressive</strong> LM和<strong>Autoencoding</strong>两类，autoregressive model在统计学里指一类随机过程，这类随机过程里我们可以根据过去的值和序列去预测将来的序列行为，放在语言学里也就是语言模型，根据上文或者下文去预测下一个词，像ELMo、GPT都属于典型的AR；而另一类autoencoding和autoencoder模型的概念有点类似，是想要去重建原始序列，类似BERT，它把原序列一些词汇mask掉然后重建原序列。但归根结底，这些预训练任务都是利用词汇的上下文词汇去预测词汇，也和 <strong>词汇的语义由它周围的其他词的分布决定</strong> 的理论相同，可以理解成广义上的语言模型。</p>
<h2 id="主流预训练模型之间的区别"><a href="#主流预训练模型之间的区别" class="headerlink" title="主流预训练模型之间的区别"></a>主流预训练模型之间的区别</h2><p>刚才提到预训练的主要区别就在预训练任务和模型的选择上，那么我们进行一个简单的比较。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>模型</th>
<th>预训练任务</th>
<th>特征提取器</th>
<th>参数量</th>
<th>训练数据集</th>
<th>训练时间</th>
</tr>
</thead>
<tbody>
<tr>
<td>ELMo</td>
<td>双向语言模型</td>
<td>Bi-LSTM</td>
<td>93.6M</td>
<td>1 billion(1B Word Benchmark)</td>
<td>无</td>
</tr>
<tr>
<td>GPT</td>
<td>单向语言模型</td>
<td>Transformer</td>
<td>110M</td>
<td>1 billion(BookCorpus dataset)</td>
<td>无</td>
</tr>
<tr>
<td>BERT</td>
<td>mask language model</td>
<td>Transformer</td>
<td>110M/340M</td>
<td>3.3 billion(BooksCorpus and Wikipedia)</td>
<td>16\64 TPU chips 4 days</td>
</tr>
<tr>
<td>XLNet</td>
<td>permutation language model</td>
<td>Transformer-xl</td>
<td>110M/340M</td>
<td>3.3billion(base) / 32.89 billion(large)</td>
<td>512 TPU v3 chips 2.5 days(large)</td>
</tr>
</tbody>
</table>
</div>
<p>简单的总结，预训练任务上选择双向语言模型是很重要的，这让模型可以从上下文学到很多信息，像mask lm和permutation lm其实都是双向语言模型的变种，可以同时利用上文和下文的信息。而特征处理器上，不用说，transformer独占鳌头。最后，BERT和XLNet的参数量都很大，训练的语料库都很大，因此训练时长也很长，XLNet-large的语料库达到了惊人的32.89billion，花费的训练资源也是惊人。不说了，更大更深更强的模型，更适合的预训练任务以及更多的训练数据，这些都很重要。</p>
<h2 id="ELMo-Deep-contextualized-representations"><a href="#ELMo-Deep-contextualized-representations" class="headerlink" title="ELMo(Deep contextualized representations)"></a>ELMo(Deep contextualized representations)</h2>
      
    </div>
    
    
    

  <div>
    
      <div>
    
        <div style="text-align:center;color: #555;font-size:14px;">-------------The End-------------</div>
    
</div>

    
  </div>

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/nlp/" rel="tag"># nlp</a>
          
            <a href="/tags/pretraining/" rel="tag"># pretraining</a>
          
            <a href="/tags/ELMo/" rel="tag"># ELMo</a>
          
            <a href="/tags/GPT/" rel="tag"># GPT</a>
          
            <a href="/tags/BERT/" rel="tag"># BERT</a>
          
            <a href="/tags/XLNet/" rel="tag"># XLNet</a>
          
            <a href="/tags/ALBert/" rel="tag"># ALBert</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/08/19/flowqa/" rel="next" title="论文分享----FlowQA">
                <i class="fa fa-chevron-left"></i> 论文分享----FlowQA
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image" src="/images/xiongmao.jpeg" alt="北冥有深">
          <p class="site-author-name" itemprop="name">北冥有深</p>
           
              <p class="site-description motion-element" itemprop="description">这世界一切既然都在变 <br> 变动中人世乘除 <br>自然就有些 <br> 近于偶然与凑巧的事情发生<br> 哀乐与悲欢<br> 都有他独特的式样</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives/">
                <span class="site-state-item-count">11</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">5</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">27</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#1-背景介绍"><span class="nav-text">1.背景介绍</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Co-occurence-matrix"><span class="nav-text">Co-occurence matrix</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Tf-idf-模型"><span class="nav-text">Tf-idf 模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Word2vec"><span class="nav-text">Word2vec</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2-后词向量时代"><span class="nav-text">2.后词向量时代</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#主流预训练模型之间的区别"><span class="nav-text">主流预训练模型之间的区别</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ELMo-Deep-contextualized-representations"><span class="nav-text">ELMo(Deep contextualized representations)</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

      <div id="music163player">
       <iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="280" height="86" src="//music.163.com/outchain/player?type=2&id=494858498&auto=1&height=66"></iframe>
      </div>
    </div>
    
  </aside>
  




        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">北冥有深</span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Gemini
  </a>
</div>


        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>

  
  <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.2"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.2"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.2"></script>



  


  




	





  





  






  





  

  

  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
  


  

  

</body>
</html>
